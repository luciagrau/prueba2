{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data from Close CRM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from closeio_api import Client\n",
    "from google.cloud import bigquery\n",
    "import datetime\n",
    "import pandas\n",
    "import pytz\n",
    "import os\n",
    "import pyarrow\n",
    "import requests\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloseApiWrapper(Client):\n",
    "    \"\"\"\n",
    "    Close API wrapper that makes it easier to paginate through resources and get all items\n",
    "    with a single function call alongside some convenience functions (e.g. getting all lead statuses).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, api_key=None, tz_offset=None, max_retries=5, development=False\n",
    "    ):\n",
    "        super().__init__(\n",
    "            api_key=api_key,\n",
    "            tz_offset=tz_offset,\n",
    "            max_retries=max_retries,\n",
    "            development=development,\n",
    "        )\n",
    "\n",
    "    def get_lead_statuses(self):\n",
    "        organization_id = self.get('me')['organizations'][0]['id']\n",
    "        return self.get(\n",
    "            f\"organization/{organization_id}\",\n",
    "            params={\"_fields\": \"lead_statuses\"},\n",
    "        )[\"lead_statuses\"]\n",
    "\n",
    "    def get_opportunity_pipelines(self):\n",
    "        organization_id = self.get('me')['organizations'][0]['id']\n",
    "        return self.get(\n",
    "            f\"organization/{organization_id}\",\n",
    "            params={\"_fields\": \"pipelines\"},\n",
    "        )[\"pipelines\"]\n",
    "\n",
    "    def get_custom_fields(self, type):\n",
    "        return self.get(f\"custom_field_schema/{type}\")[\"fields\"]\n",
    "\n",
    "    def get_opportunity_statuses(self):\n",
    "        organization_id = self.get('me')['organizations'][0]['id']\n",
    "        pipelines = self.get(\n",
    "            f\"organization/{organization_id}\",\n",
    "            params={\"_fields\": \"pipelines\"},\n",
    "        )[\"pipelines\"]\n",
    "\n",
    "        opportunity_statuses = []\n",
    "        for pipeline in pipelines:\n",
    "            opportunity_statuses.extend(pipeline['statuses'])\n",
    "\n",
    "        return opportunity_statuses\n",
    "\n",
    "    def get_all_items(self, url, params=None):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "\n",
    "        items = []\n",
    "        has_more = True\n",
    "        offset = 0\n",
    "        while has_more:\n",
    "            params[\"_skip\"] = offset\n",
    "            resp = self.get(url, params=params)\n",
    "            items.extend(resp['data'])\n",
    "            offset += len(resp[\"data\"])\n",
    "            has_more = resp[\"has_more\"]\n",
    "\n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'limit': None,\n",
       " 'query': {'negate': False,\n",
       "  'queries': [{'negate': False, 'object_type': 'lead', 'type': 'object_type'},\n",
       "   {'negate': False,\n",
       "    'queries': [{'negate': False,\n",
       "      'queries': [{'condition': {'type': 'term',\n",
       "         'values': ['Agora',\n",
       "          'Cover Manager',\n",
       "          'Dish - Makro',\n",
       "          'El Tenedor',\n",
       "          'Five stars restaurant reservation',\n",
       "          'Hospitality Digital',\n",
       "          'None',\n",
       "          'Resmio',\n",
       "          'Restoo',\n",
       "          'Sevenrooms',\n",
       "          'Spotlinker',\n",
       "          'Superb']},\n",
       "        'field': {'custom_field_id': 'cf_0pYIi4FHHnhOM1ifbXJYZLcfDU13bEH9eHYslzr1Zab',\n",
       "         'type': 'custom_field'},\n",
       "        'negate': False,\n",
       "        'type': 'field_condition'},\n",
       "       {'condition': {'object_ids': ['stat_8t23pHs9IQotTZunnUeWB6w6RD2Na1Dob5fk9eAsLUd',\n",
       "          'stat_9xQf6cjuUwuQ92zPUmUnakvs5YhjUDtvkQBzenguCET',\n",
       "          'stat_Gb5oaZjacOsnVKwNZFdDODIsSTipR1HTAFwBfWPziKG',\n",
       "          'stat_L7NEk2Kc2pX7WMsBQn07Vs3KTwG3lkp70p0446VZsTY',\n",
       "          'stat_MD0TdHBJkRjxaCfemqLSTgwmLC7FQeaxt7Po9R1Ng04',\n",
       "          'stat_NeP2VGE2PASFs95QHEnxEhXi382RFX2fDPjCK29If1r',\n",
       "          'stat_PSdJsfSHu9Wf73moypWSQpNITASJ3W3Tzi0VU88Nsjy',\n",
       "          'stat_XdRA89GRcYWzd4q14iCXIEiue0Po6Us4G68b2t8GT4v',\n",
       "          'stat_cfQPZ3aedcrxwaDUrK37oXzIRwMUKsODlSBJWLYlyHh',\n",
       "          'stat_hWNOcRMXFrdlQECF0itYxqxzYs2qmR6OeQepEWhu1YM',\n",
       "          'stat_iEgcVYfhfypOoqZ87h08TLBmpAplUPFpGNYut2bESGR',\n",
       "          'stat_wnopZFSHmC9PCVDHgym1wrpEUNT0XQzF1TJ8Uu3WIlN'],\n",
       "         'reference_type': 'status.lead',\n",
       "         'type': 'reference'},\n",
       "        'field': {'field_name': 'status_id',\n",
       "         'object_type': 'lead',\n",
       "         'type': 'regular_field'},\n",
       "        'negate': False,\n",
       "        'type': 'field_condition'},\n",
       "       {'condition': {'type': 'exists'},\n",
       "        'field': {'field_name': 'name',\n",
       "         'object_type': 'lead',\n",
       "         'type': 'regular_field'},\n",
       "        'negate': False,\n",
       "        'type': 'field_condition'}],\n",
       "      'type': 'and'}],\n",
       "    'type': 'and'}],\n",
       "  'type': 'and'},\n",
       " 'results_limit': None,\n",
       " 'sort': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtro='''{\n",
    "    \"limit\": null,\n",
    "    \"query\": {\n",
    "        \"negate\": false,\n",
    "        \"queries\": [\n",
    "            {\n",
    "                \"negate\": false,\n",
    "                \"object_type\": \"lead\",\n",
    "                \"type\": \"object_type\"\n",
    "            },\n",
    "            {\n",
    "                \"negate\": false,\n",
    "                \"queries\": [\n",
    "                    {\n",
    "                        \"negate\": false,\n",
    "                        \"queries\": [\n",
    "                            {\n",
    "                                \"condition\": {\n",
    "                                    \"type\": \"term\",\n",
    "                                    \"values\": [\n",
    "                                        \"Agora\",\n",
    "                                        \"Cover Manager\",\n",
    "                                        \"Dish - Makro\",\n",
    "                                        \"El Tenedor\",\n",
    "                                        \"Five stars restaurant reservation\",\n",
    "                                        \"Hospitality Digital\",\n",
    "                                        \"None\",\n",
    "                                        \"Resmio\",\n",
    "                                        \"Restoo\",\n",
    "                                        \"Sevenrooms\",\n",
    "                                        \"Spotlinker\",\n",
    "                                        \"Superb\"\n",
    "                                    ]\n",
    "                                },\n",
    "                                \"field\": {\n",
    "                                    \"custom_field_id\": \"cf_0pYIi4FHHnhOM1ifbXJYZLcfDU13bEH9eHYslzr1Zab\",\n",
    "                                    \"type\": \"custom_field\"\n",
    "                                },\n",
    "                                \"negate\": false,\n",
    "                                \"type\": \"field_condition\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"condition\": {\n",
    "                                    \"object_ids\": [\n",
    "                                        \"stat_8t23pHs9IQotTZunnUeWB6w6RD2Na1Dob5fk9eAsLUd\",\n",
    "                                        \"stat_9xQf6cjuUwuQ92zPUmUnakvs5YhjUDtvkQBzenguCET\",\n",
    "                                        \"stat_Gb5oaZjacOsnVKwNZFdDODIsSTipR1HTAFwBfWPziKG\",\n",
    "                                        \"stat_L7NEk2Kc2pX7WMsBQn07Vs3KTwG3lkp70p0446VZsTY\",\n",
    "                                        \"stat_MD0TdHBJkRjxaCfemqLSTgwmLC7FQeaxt7Po9R1Ng04\",\n",
    "                                        \"stat_NeP2VGE2PASFs95QHEnxEhXi382RFX2fDPjCK29If1r\",\n",
    "                                        \"stat_PSdJsfSHu9Wf73moypWSQpNITASJ3W3Tzi0VU88Nsjy\",\n",
    "                                        \"stat_XdRA89GRcYWzd4q14iCXIEiue0Po6Us4G68b2t8GT4v\",\n",
    "                                        \"stat_cfQPZ3aedcrxwaDUrK37oXzIRwMUKsODlSBJWLYlyHh\",\n",
    "                                        \"stat_hWNOcRMXFrdlQECF0itYxqxzYs2qmR6OeQepEWhu1YM\",\n",
    "                                        \"stat_iEgcVYfhfypOoqZ87h08TLBmpAplUPFpGNYut2bESGR\",\n",
    "                                        \"stat_wnopZFSHmC9PCVDHgym1wrpEUNT0XQzF1TJ8Uu3WIlN\"\n",
    "                                    ],\n",
    "                                    \"reference_type\": \"status.lead\",\n",
    "                                    \"type\": \"reference\"\n",
    "                                },\n",
    "                                \"field\": {\n",
    "                                    \"field_name\": \"status_id\",\n",
    "                                    \"object_type\": \"lead\",\n",
    "                                    \"type\": \"regular_field\"\n",
    "                                },\n",
    "                                \"negate\": false,\n",
    "                                \"type\": \"field_condition\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"condition\": {\n",
    "                                    \"type\": \"exists\"\n",
    "                                },\n",
    "                                \"field\": {\n",
    "                                    \"field_name\": \"name\",\n",
    "                                    \"object_type\": \"lead\",\n",
    "                                    \"type\": \"regular_field\"\n",
    "                                },\n",
    "                                \"negate\": false,\n",
    "                                \"type\": \"field_condition\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"type\": \"and\"\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"and\"\n",
    "            }\n",
    "        ],\n",
    "        \"type\": \"and\"\n",
    "    },\n",
    "    \"results_limit\": null,\n",
    "    \"sort\": []\n",
    "}\n",
    "'''\n",
    "\n",
    "filtro= json.loads(filtro) #cargar el json y lo convierte en diccionario de python\n",
    "filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "has_more = True\n",
    "offset = 0\n",
    "calls = []\n",
    "display_names = {}\n",
    "api = CloseApiWrapper(\"api_3yFWGD0paiQ2UKvsQvrahS.6ALHM6ZnMvbnfv0jTpjA7Y\")\n",
    "resp=api.get( \n",
    "        'lead',\n",
    "        params={\n",
    "                '_skip':offset,\n",
    "                'query': filtro,\n",
    "                '_fields': 'id,display_name,status_label'\n",
    "            },)\n",
    "\n",
    "resp2=resp.get(\"data\") ##esto es una lista\n",
    "df=pd.DataFrame()\n",
    "\n",
    "for i in resp2:\n",
    "    df=pd.DataFrame(pd.concat([df,pd.DataFrame(i.values(), index=i.keys()).transpose().set_index(\"id\")]))\n",
    "\n",
    "print(df)\n",
    "#df.to_csv(r'C:\\Users\\jaime\\Downloads\\Bookline\\prueba.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subir datos a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrowNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading pyarrow-11.0.0-cp38-cp38-win_amd64.whl (20.6 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\jaime\\anaconda3\\lib\\site-packages (from pyarrow) (1.18.5)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-11.0.0\n"
     ]
    }
   ],
   "source": [
    "pip install -- pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-3.5.0-py2.py3-none-any.whl (215 kB)\n",
      "Collecting grpcio<2.0dev,>=1.47.0\n",
      "  Downloading grpcio-1.51.1-cp38-cp38-win_amd64.whl (3.7 MB)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0.0 in c:\\users\\jaime\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (20.4)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.15.0\n",
      "  Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0dev,>=2.7.2 in c:\\users\\jaime\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.21.0 in c:\\users\\jaime\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.24.0)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.21.12-cp38-cp38-win_amd64.whl (527 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in c:\\users\\jaime\\anaconda3\\lib\\site-packages (from packaging>=20.0.0->google-cloud-bigquery) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\jaime\\anaconda3\\lib\\site-packages (from packaging>=20.0.0->google-cloud-bigquery) (1.15.0)\n",
      "Collecting google-auth<3.0dev,>=2.14.1\n",
      "  Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2; extra == \"grpc\"\n",
      "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\jaime\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jaime\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\jaime\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\jaime\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.0.4)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-win_amd64.whl (27 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: grpcio, protobuf, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, googleapis-common-protos, grpcio-status, google-api-core, proto-plus, google-cloud-core, google-crc32c, google-resumable-media, google-cloud-bigquery\n",
      "Successfully installed cachetools-5.3.0 google-api-core-2.11.0 google-auth-2.16.0 google-cloud-bigquery-3.5.0 google-cloud-core-2.3.2 google-crc32c-1.5.0 google-resumable-media-2.4.1 googleapis-common-protos-1.58.0 grpcio-1.51.1 grpcio-status-1.51.1 proto-plus-1.22.2 protobuf-4.21.12 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/looker-studio-377715/jobs?uploadType=multipart: Not found: Dataset looker-studio-377715:looker-studio-377715",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidResponse\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\client.py\u001b[0m in \u001b[0;36mload_table_from_file\u001b[1;34m(self, file_obj, destination, rewind, size, num_retries, job_id, job_id_prefix, location, project, job_config, timeout)\u001b[0m\n\u001b[0;32m   2455\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2456\u001b[1;33m                 response = self._do_multipart_upload(\n\u001b[0m\u001b[0;32m   2457\u001b[0m                     \u001b[0mfile_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_retries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\client.py\u001b[0m in \u001b[0;36m_do_multipart_upload\u001b[1;34m(self, stream, metadata, size, num_retries, timeout, project)\u001b[0m\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3026\u001b[1;33m         response = upload.transmit(\n\u001b[0m\u001b[0;32m   3027\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_http\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_GENERIC_CONTENT_TYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\resumable_media\\requests\\upload.py\u001b[0m in \u001b[0;36mtransmit\u001b[1;34m(self, transport, data, metadata, content_type, timeout)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         return _request_helpers.wait_and_retry(\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[0mretriable_request\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_status_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_retry_strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\resumable_media\\requests\\_request_helpers.py\u001b[0m in \u001b[0;36mwait_and_retry\u001b[1;34m(func, get_status_code, retry_strategy)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_CONNECTION_ERROR_CLASSES\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\resumable_media\\requests\\upload.py\u001b[0m in \u001b[0;36mretriable_request\u001b[1;34m()\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\resumable_media\\_upload.py\u001b[0m in \u001b[0;36m_process_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_finished\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0m_helpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_status_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_status_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\resumable_media\\_helpers.py\u001b[0m in \u001b[0;36mrequire_status_code\u001b[1;34m(response, status_codes, get_status_code, callback)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         raise common.InvalidResponse(\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidResponse\u001b[0m: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-201-e17656af6b9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m job = client.load_table_from_dataframe(\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjob_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m ) \n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\client.py\u001b[0m in \u001b[0;36mload_table_from_dataframe\u001b[1;34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\u001b[0m\n\u001b[0;32m   2705\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmppath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtmpfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2706\u001b[0m                 \u001b[0mfile_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmppath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2707\u001b[1;33m                 return self.load_table_from_file(\n\u001b[0m\u001b[0;32m   2708\u001b[0m                     \u001b[0mtmpfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2709\u001b[0m                     \u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\client.py\u001b[0m in \u001b[0;36mload_table_from_file\u001b[1;34m(self, file_obj, destination, rewind, size, num_retries, job_id, job_id_prefix, location, project, job_config, timeout)\u001b[0m\n\u001b[0;32m   2458\u001b[0m                 )\n\u001b[0;32m   2459\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mresumable_media\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidResponse\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2460\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2462\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtyping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLoadJob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_from_resource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFound\u001b[0m: 404 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/looker-studio-377715/jobs?uploadType=multipart: Not found: Dataset looker-studio-377715:looker-studio-377715"
     ]
    }
   ],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/jaime/Downloads/Bookline/looker-studio-377715-075aecd69bce.json\"\n",
    "client = bigquery.Client()\n",
    "table_id= \"looker-studio-377715.restaurantes\" #esto lo he sacado de big query\n",
    "dataframe=df\n",
    "\n",
    "\n",
    "##Aqui ya hacer el query\n",
    "## se puede tmb hacer un index \n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    # Specify a (partial) schema. All columns are always written to the\n",
    "    # table. The schema is used to assist in data type definitions.\n",
    "    schema=[\n",
    "        \n",
    "        bigquery.SchemaField(\"id\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"display_name\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"status_label\", bigquery.enums.SqlTypeNames.STRING),\n",
    "    \n",
    "        \n",
    "    ],\n",
    "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "    # disposition it replaces the table with the loaded data.\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(\n",
    "    dataframe, table_id, job_config=job_config\n",
    ") \n",
    "\n",
    "\n",
    " \n",
    "#tmb me da error de table id primero q no esta bien hecho, lo cambio y me dice que no esta en formato SQL. Y al ponerlo en formato SQL me da error \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "job.result()  # Wait for the job to complete.\n",
    "\n",
    "table = client.get_table(table_id)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        table.num_rows, len(table.schema), table_id\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
